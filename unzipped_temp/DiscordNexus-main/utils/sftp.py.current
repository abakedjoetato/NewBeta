"""
SFTP utility functions for server connections and file operations
"""
import os
import re
import logging
import asyncio
import datetime
from io import StringIO

import paramiko
from paramiko.ssh_exception import SSHException, AuthenticationException

from config import SFTP_CONNECTION_SETTINGS, CSV_FILENAME_PATTERN, LOG_FILENAME

logger = logging.getLogger(__name__)

class SFTPClient:
    """SFTP client for connecting to game servers and retrieving files"""
    
    def __init__(self, host, port, username, password, server_id):
        """Initialize SFTP client with connection parameters"""
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.server_id = server_id
        self.client = None
        self.sftp = None
        self.root_path = None
        self.connected = False
        self.last_error = None
    
    async def connect(self):
        """Establish SFTP connection"""
        try:
            # Create SSH client
            self.client = paramiko.SSHClient()
            self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            
            # Connect to server
            self.client.connect(
                hostname=self.host,
                port=self.port,
                username=self.username,
                password=self.password,
                **SFTP_CONNECTION_SETTINGS
            )
            
            # Open SFTP session
            self.sftp = self.client.open_sftp()
            logger.info(f"Connected to SFTP server: {self.host}:{self.port} for server {self.server_id}")
            
            # Try to find the appropriate root directory for this server
            logger.info(f"Searching for server directory for server ID: {self.server_id}")
            await self.find_root_path()
            
            self.connected = True
            self.last_error = None
            return True
            
        except AuthenticationException:
            self.last_error = "Authentication failed. Check username and password."
            logger.error(f"Authentication failed for SFTP server: {self.host}:{self.port}")
            self.connected = False
            return False
            
        except SSHException as e:
            self.last_error = f"SSH error: {str(e)}"
            logger.error(f"SSH error connecting to {self.host}:{self.port}: {e}")
            self.connected = False
            return False
            
        except Exception as e:
            self.last_error = f"Connection error: {str(e)}"
            logger.error(f"Error connecting to SFTP server {self.host}:{self.port}: {e}", exc_info=True)
            self.connected = False
            return False
    
    async def find_root_path(self):
        """Find the root path containing host_serverID or IP_serverID pattern"""
        try:
            # Start from current directory
            current_path = '.'
            self.root_path = None
            
            # Extract IP address from host (remove port if present)
            ip_address = self.host.split(':')[0]
            
            # Create patterns to try (both host_ pattern and IP_ pattern)
            patterns = [
                f"{ip_address}_{self.server_id}",   # IP_serverID format (e.g., "79.127.236.1_7020")
                f"host_{self.server_id}",           # Legacy host_serverID format
                f"{self.server_id}"                 # Server ID directly
            ]
            
            logger.info(f"Searching for directory patterns: {patterns}")
            
            # First try: Direct search in the root directory (no recursion)
            try:
                items = self.sftp.listdir(current_path)
                logger.info(f"Found {len(items)} items in root directory: {', '.join(items[:10])}{'...' if len(items) > 10 else ''}")
                
                for item in items:
                    # Check if the item name matches our patterns
                    item_lower = item.lower()
                    for pattern in patterns:
                        if pattern.lower() in item_lower:
                            self.root_path = os.path.join(current_path, item)
                            logger.info(f"Found matching directory: {self.root_path}")
                            return
                    
                    # Also check for the server ID directly within the name
                    if str(self.server_id) in item:
                        self.root_path = os.path.join(current_path, item)
                        logger.info(f"Found matching directory with server ID: {self.root_path}")
                        return
            except Exception as dir_e:
                logger.error(f"Error listing root directory: {dir_e}")
            
            # Second try: Look for common directory structures
            common_paths = [
                f"./host_{self.server_id}",
                f"./{ip_address}_{self.server_id}",
                f"./server_{self.server_id}",
                f"./gameserver_{self.server_id}",
                f"./deadside_{self.server_id}"
            ]
            
            for path in common_paths:
                try:
                    # Check if the path exists
                    self.sftp.stat(path)
                    self.root_path = path
                    logger.info(f"Found server directory with common path: {self.root_path}")
                    return
                except Exception:
                    # Continue to the next path
                    pass
            
            # Third try: If still no match, fall back to recursive search (limited depth)
            if not self.root_path:
                logger.info("Attempting recursive search for server directory")
                for pattern in patterns:
                    await self._find_path_recursive(current_path, pattern, max_depth=2)
                    if self.root_path:
                        logger.info(f"Found matching directory with recursive search: {self.root_path}")
                        return
            
            # Last resort: Use first directory we find that might contain valid files
            if not self.root_path:
                # Try to find any directory that contains CSV files
                logger.info("Last resort: searching for directories with CSV files")
                try:
                    for item in items:
                        try:
                            test_path = os.path.join(current_path, item)
                            # Check if it's a directory
                            if self._is_dir(test_path):
                                # Try to find paths with CSV files
                                potential_paths = [
                                    test_path,
                                    os.path.join(test_path, "actual1"),
                                    os.path.join(test_path, "actual1", "deathlogs")
                                ]
                                
                                for path in potential_paths:
                                    try:
                                        # Check if this path exists and has CSV files
                                        path_items = self.sftp.listdir(path)
                                        for pi in path_items:
                                            if pi.endswith('.csv'):
                                                self.root_path = test_path
                                                logger.info(f"Found directory with CSV files: {test_path}")
                                                return
                                    except Exception:
                                        # Continue to the next potential path
                                        pass
                        except Exception:
                            # Continue to the next item
                            pass
                except Exception as e:
                    logger.error(f"Error in last resort directory search: {e}")
                        
            if not self.root_path:
                logger.warning(f"Could not find root path with any pattern. Using current directory.")
                self.root_path = '.'
                
        except Exception as e:
            logger.error(f"Error finding root path: {e}", exc_info=True)
            self.root_path = '.'
    
    async def _find_path_recursive(self, path, pattern, max_depth=3, current_depth=0):
        """Recursively search for a directory matching the pattern"""
        if current_depth > max_depth:
            return
        
        try:
            # Check if current directory name matches pattern
            if pattern in os.path.basename(path):
                self.root_path = path
                logger.info(f"Found root path: {path}")
                return
            
            # List directory contents
            items = self.sftp.listdir(path)
            
            # Check subdirectories
            for item in items:
                item_path = os.path.join(path, item)
                try:
                    # Check if item is a directory
                    if self._is_dir(item_path):
                        # Check if directory name contains pattern
                        if pattern in item:
                            self.root_path = item_path
                            logger.info(f"Found root path: {item_path}")
                            return
                        
                        # Recursively check subdirectory
                        await self._find_path_recursive(
                            item_path, pattern, max_depth, current_depth + 1
                        )
                        
                        # If root path was found in recursive call, return
                        if self.root_path:
                            return
                except:
                    # Skip if can't check directory
                    continue
                    
        except Exception as e:
            logger.error(f"Error searching directory {path}: {e}")
    
    def _is_dir(self, path):
        """Check if a path is a directory with timeout protection"""
        try:
            # Use a very short timeout for this operation since it's called frequently
            # If it takes too long, just assume it's not a directory
            start_time = asyncio.get_event_loop().time()
            result = self.sftp.stat(path).st_mode & 0o40000 != 0
            end_time = asyncio.get_event_loop().time()
            
            # Log if this operation is taking too long (helping us diagnose issues)
            if end_time - start_time > 0.5:  # More than 500ms is suspicious
                logger.warning(f"_is_dir operation on {path} took {(end_time - start_time):.2f} seconds")
                
            return result
        except Exception as e:
            # Be more specific in logging to help diagnose issues
            if str(e) != "No such file":  # Don't log common "No such file" errors
                logger.debug(f"Error in _is_dir for {path}: {e}")
            return False
    
    async def disconnect(self):
        """Close SFTP connection"""
        try:
            if self.sftp:
                self.sftp.close()
            if self.client:
                self.client.close()
            self.connected = False
            logger.info(f"Disconnected from SFTP server: {self.host}:{self.port}")
        except Exception as e:
            logger.error(f"Error disconnecting from SFTP server: {e}")
    
    async def get_latest_csv_file(self):
        """Get the path to the latest CSV file by searching ALL directories and subdirectories"""
        # Record start time to prevent heartbeat blocks
        start_time = asyncio.get_event_loop().time()
        
        if not self.connected:
            logger.info("SFTP client not connected, attempting to connect")
            await self.connect()
            
        if not self.connected:
            logger.error("Failed to connect to SFTP server")
            return None
        
        try:
            csv_files = []
            import time
            
            # Base path for the server: {Host}_{serverid}
            server_dir_pattern = f"{self.host.split(':')[0]}_{self.server_id}"
            server_base_path = os.path.join(".", server_dir_pattern)
            
            # Our primary target is actual1 and paths forward into it
            actual1_path = os.path.join(server_base_path, "actual1")
            
            # Forward search paths - specific to more general
            forward_search_paths = [
                # First path to check: the primary deathlogs directory
                os.path.join(actual1_path, "deathlogs"),
                
                # These are potential world subdirectories of deathlogs - include ALL possible worlds
                os.path.join(actual1_path, "deathlogs", "world_0"),
                os.path.join(actual1_path, "deathlogs", "world_1"),
                os.path.join(actual1_path, "deathlogs", "world_2"),
                os.path.join(actual1_path, "deathlogs", "world_3"),
                os.path.join(actual1_path, "deathlogs", "world_4"),
                os.path.join(actual1_path, "deathlogs", "world_5"),
                os.path.join(actual1_path, "deathlogs", "world_6"),
                os.path.join(actual1_path, "deathlogs", "world_7"),
                os.path.join(actual1_path, "deathlogs", "world_8"),
                os.path.join(actual1_path, "deathlogs", "world_9"),
                
                # Then regular subdirectories of actual1 that might contain CSV files
                os.path.join(actual1_path, "world_0"),
                os.path.join(actual1_path, "world_1"),
                os.path.join(actual1_path, "world_2"),
                os.path.join(actual1_path, "world_3"),
                os.path.join(actual1_path, "world_4"),
                os.path.join(actual1_path, "world_5"),
                os.path.join(actual1_path, "world_6"),
                os.path.join(actual1_path, "world_7"),
                os.path.join(actual1_path, "world_8"),
                os.path.join(actual1_path, "world_9"),
                
                # Finally, check actual1 itself as last resort
                actual1_path,
            ]
            
            logger.info(f"Starting thorough search for CSV files across ALL world subdirectories")
            
            # Check each potential path in order for CSV files - always check all paths
            for search_path in forward_search_paths:
                # Check for timeout
                if asyncio.get_event_loop().time() - start_time > 30:
                    logger.warning("CSV search taking too long, stopping to prevent heartbeat timeout")
                    break
                
                try:
                    # Check if this path exists
                    async def check_path_exists():
                        try:
                            files = await asyncio.to_thread(lambda: self.sftp.listdir(search_path))
                            return True, files
                        except Exception as e:
                            return False, None
                    
                    path_exists, files = await asyncio.wait_for(check_path_exists(), timeout=3.0)
                    
                    if path_exists and files:
                        logger.info(f"Checking path: {search_path}")
                        
                        # CRITICAL FIX: Special handling for deathlogs directory
                        # If this is the deathlogs directory and contains world_X subdirectories, check them directly
                        if search_path.endswith("deathlogs"):
                            world_dirs = [f for f in files if f.startswith("world_")]
                            if world_dirs:
                                logger.info(f"DIRECT CHECK: Found {len(world_dirs)} world directories in deathlogs: {world_dirs}")
                                
                                for world_dir in world_dirs:
                                    world_path = os.path.join(search_path, world_dir)
                                    logger.info(f"DIRECT CHECK: Checking world directory: {world_path}")
                                    
                                    try:
                                        # List files in world directory
                                        world_files = await asyncio.to_thread(lambda: self.sftp.listdir(world_path))
                                        logger.info(f"DIRECT CHECK: World directory {world_dir} contains {len(world_files)} files")
                                        
                                        # Check for CSV files
                                        for world_file in world_files:
                                            if world_file.lower().endswith('.csv'):
                                                world_file_path = os.path.join(world_path, world_file)
                                                logger.info(f"DIRECT CHECK: Found CSV file in world dir: {world_path}/{world_file}")
                                                
                                                try:
                                                    # Get timestamp
                                                    attrs = await asyncio.to_thread(lambda: self.sftp.stat(world_file_path))
                                                    mtime = attrs.st_mtime
                                                    logger.info(f"DIRECT CHECK: CSV file timestamp: {mtime}")
                                                    csv_files.append((world_file_path, mtime, world_file))
                                                except Exception:
                                                    # Use current time as fallback
                                                    logger.info(f"DIRECT CHECK: Using current time for {world_file}")
                                                    csv_files.append((world_file_path, time.time(), world_file))
                                    except Exception as we:
                                        logger.warning(f"DIRECT CHECK: Error checking world dir {world_path}: {we}")
                        
                        # Standard check for CSV files in the current directory
                        found_csv = False
                        for filename in files:
                            if re.match(CSV_FILENAME_PATTERN, filename):
                                found_csv = True
                                file_path = os.path.join(search_path, filename)
                                logger.info(f"Found CSV file: {filename} in {search_path}")
                                
                                # Add to our list with simple timestamp
                                try:
                                    # Try to get real timestamp, but don't block too long
                                    async def get_file_stat():
                                        return await asyncio.to_thread(lambda: self.sftp.stat(file_path))
                                    
                                    attrs = await asyncio.wait_for(get_file_stat(), timeout=1.0)
                                    mtime = attrs.st_mtime
                                    csv_files.append((file_path, mtime, filename))
                                except Exception:
                                    # Use current time as fallback
                                    csv_files.append((file_path, time.time(), filename))
                        
                        if found_csv:
                            logger.info(f"Found CSV files in {search_path}")
                            # Continue searching - don't break or return early!
                            
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout checking path: {search_path}")
                    continue
                except Exception as e:
                    logger.debug(f"Error checking path {search_path}: {e}")
                    continue
            
            # Even if we found files, still try a focused recursive search for any we might have missed
            # This ensures we check ALL possible locations for CSV files
            if asyncio.get_event_loop().time() - start_time < 25:
                logger.info("Checking for additional CSV files with focused recursive search")
                
                # Do a limited recursive search of actual1, looking specifically for map worlds
                try:
                    if await self._path_exists(actual1_path):
                        world_dirs = []
                        
                        # Look for any world_X directories in actual1
                        async def list_actual1():
                            return await asyncio.to_thread(lambda: self.sftp.listdir(actual1_path))
                            
                        actual1_files = await asyncio.wait_for(list_actual1(), timeout=2.0)
                        
                        # Find any potential world directories that we haven't explicitly checked
                        for item in actual1_files:
                            if item.lower().startswith("world_") and await self._is_dir_async(os.path.join(actual1_path, item)):
                                world_dir = os.path.join(actual1_path, item)
                                # Only add if not already in our forward search paths
                                if world_dir not in forward_search_paths:
                                    world_dirs.append(world_dir)
                        
                        # Also check for deathlogs directory if it exists, and search its world folders
                        deathlogs_path = os.path.join(actual1_path, "deathlogs")
                        if await self._path_exists(deathlogs_path):
                            async def list_deathlogs():
                                return await asyncio.to_thread(lambda: self.sftp.listdir(deathlogs_path))
                                
                            try:
                                deathlogs_files = await asyncio.wait_for(list_deathlogs(), timeout=2.0)
                                
                                # Find world directories in deathlogs that we haven't explicitly checked
                                for item in deathlogs_files:
                                    if item.lower().startswith("world_") and await self._is_dir_async(os.path.join(deathlogs_path, item)):
                                        world_dir = os.path.join(deathlogs_path, item)
                                        # Only add if not already in our forward search paths
                                        if world_dir not in forward_search_paths:
                                            world_dirs.append(world_dir)
                            except Exception:
                                pass
                                
                        # Search all newly discovered world directories for CSV files
                        for world_dir in world_dirs:
                            if asyncio.get_event_loop().time() - start_time > 30:
                                logger.warning("Additional CSV search taking too long, stopping to prevent heartbeat timeout")
                                break
                                
                            try:
                                world_files = await asyncio.wait_for(
                                    asyncio.to_thread(lambda: self.sftp.listdir(world_dir)), 
                                    timeout=2.0
                                )
                                
                                # Check for CSV files
                                found_csv = False
                                for filename in world_files:
                                    if re.match(CSV_FILENAME_PATTERN, filename):
                                        found_csv = True
                                        file_path = os.path.join(world_dir, filename)
                                        csv_files.append((file_path, time.time(), filename))
                                
                                if found_csv:
                                    logger.info(f"Found additional CSV files in {world_dir}")
                            except Exception:
                                pass
                except Exception as e:
                    logger.debug(f"Error in world dir search: {e}")
            
            # Sort by modification time (newest first)
            # Sort with error handling to prevent type errors
            try:
                csv_files.sort(key=lambda x: x[1] if isinstance(x[1], datetime.datetime) else datetime.datetime.now(), reverse=True)
            except Exception as sort_error:
                logger.error(f"Error sorting CSV files: {sort_error}")
                # If sorting fails, continue without sorting
            
            if csv_files:
                # Return path to the latest file, but log all found files
                latest_file = csv_files[0][0]
                
                # Log summary of all found files 
                logger.info(f"Found {len(csv_files)} CSV files in total across all directories")
                # Fix for timestamp handling to prevent errors
                try:
                    timestamp = csv_files[0][1]
                    if isinstance(timestamp, (int, float)):
                        modified_time = datetime.datetime.fromtimestamp(timestamp)
                    elif isinstance(timestamp, datetime.datetime):
                        modified_time = timestamp
                    else:
                        modified_time = "Unknown timestamp format"
                    logger.info(f"Using latest CSV file: {latest_file} (Modified: {modified_time})")
                except Exception as e:
                    logger.error(f"Error formatting timestamp: {e}")
                    logger.info(f"Using latest CSV file: {latest_file} (Modified time unavailable)")
                
                # Log info about the 3 most recent files (if we have that many)
                try:
                    for i, (file_path, mtime, filename) in enumerate(csv_files[:3]):
                        # Handle different timestamp formats safely
                        try:
                            if isinstance(mtime, (int, float)):
                                timestamp_str = str(datetime.datetime.fromtimestamp(mtime))
                            elif isinstance(mtime, datetime.datetime):
                                timestamp_str = str(mtime)
                            else:
                                timestamp_str = "Unknown timestamp format"
                            logger.info(f"CSV #{i+1}: {file_path} (Modified: {timestamp_str})")
                        except Exception as e:
                            logger.warning(f"Error formatting timestamp for file {file_path}: {e}")
                            logger.info(f"CSV #{i+1}: {file_path} (Modified: unavailable)")
                except Exception as e:
                    logger.warning(f"Error processing top CSV files: {e}")
                
                return latest_file
            else:
                logger.warning(f"No CSV files found in any location for server {self.server_id}")
                return None
                
        except Exception as e:
            logger.error(f"Error getting latest CSV file: {e}", exc_info=True)
            return None
    
    async def _path_exists(self, path):
        """Check if a path exists with timeout protection"""
        try:
            await asyncio.to_thread(lambda: self.sftp.listdir(path))
            return True
        except Exception:
            return False
            
    async def _is_dir_async(self, path):
        """Check if a path is a directory with timeout protection"""
        try:
            result = await asyncio.to_thread(lambda: self._is_dir(path))
            return result
        except Exception:
            return False
            
    async def _find_csv_files_recursive_with_timeout(self, path, max_depth=2, current_depth=0, start_time=None, timeout_limit=20):
        """Recursively search for CSV files with timeout protection
        
        Args:
            path: Starting directory path
            max_depth: Maximum recursion depth
            current_depth: Current recursion depth
            start_time: Start time of the operation for timeout checking
            timeout_limit: Maximum seconds allowed for the operation
            
        Returns:
            List of CSV file paths found
        """
        # Initialize found files list
        found_files = []
        
        # Check if we've hit our depth limit
        if current_depth > max_depth:
            return found_files
            
        # Check if we're approaching timeout
        if start_time and (asyncio.get_event_loop().time() - start_time > timeout_limit):
            logger.warning(f"Recursive search taking too long at item {path}, stopping to prevent heartbeat timeout")
            return found_files
        
        try:
            # List directory contents with timeout protection
            async def list_dir_with_timeout():
                return await asyncio.to_thread(lambda: self.sftp.listdir(path))
                
            try:
                items = await asyncio.wait_for(list_dir_with_timeout(), timeout=2.0)
                
                # First scan for any CSV files in the current directory
                for item in items:
                    if re.match(CSV_FILENAME_PATTERN, item):
                        file_path = os.path.join(path, item)
                        found_files.append(file_path)
                        
                # Now check subdirectories if we haven't hit depth limit
                if current_depth < max_depth:
                    # Add a small delay between operations to avoid blocking the event loop
                    await asyncio.sleep(0.01)
                    
                    # Check for subdirectories
                    for item in items:
                        # Skip checking if we're near timeout
                        if start_time and (asyncio.get_event_loop().time() - start_time > timeout_limit):
                            logger.warning(f"Recursive search taking too long at item {item}, stopping to prevent heartbeat timeout")
                            return found_files
                        
                        item_path = os.path.join(path, item)
                        
                        try:
                            # Check if this is a directory
                            is_dir = await asyncio.to_thread(lambda: self._is_dir(item_path))
                            
                            if is_dir:
                                # Recursively search this subdirectory
                                subdir_files = await self._find_csv_files_recursive_with_timeout(
                                    item_path,
                                    max_depth,
                                    current_depth + 1,
                                    start_time,
                                    timeout_limit
                                )
                                found_files.extend(subdir_files)
                        except Exception:
                            # Skip problematic items
                            continue
                            
            except asyncio.TimeoutError:
                logger.warning(f"Timeout listing directory: {path}")
                
        except Exception as e:
            logger.debug(f"Error scanning directory {path}: {e}")
            
        return found_files
            
    async def _find_csv_files_recursive(self, path, max_depth=4, current_depth=0):
        """Recursively search for CSV files"""
        if current_depth > max_depth:
            return []
            
        found_files = []
        try:
            # List directory contents
            items = self.sftp.listdir(path)
            
            # Check each item
            for item in items:
                item_path = os.path.join(path, item)
                try:
                    if self._is_dir(item_path):
                        # Recursively check subdirectory
                        subdir_files = await self._find_csv_files_recursive(
                            item_path, max_depth, current_depth + 1
                        )
                        found_files.extend(subdir_files)
                    elif re.match(CSV_FILENAME_PATTERN, item):
                        # Found a CSV file
                        found_files.append(item_path)
                except Exception:
                    continue
        except Exception as e:
            logger.debug(f"Error scanning directory {path}: {e}")
            
        return found_files
    
    async def get_all_csv_files(self):
        """Get all CSV files sorted by timestamp (oldest first)"""
        if not self.connected:
            await self.connect()
        if not self.connected:
            return []
        
        try:
            csv_files = []
            
            # First check if the specified server directory exists
            target_directory = None
            
            # List root directory to search for our server directory
            logger.info("Searching for server directory in root...")
            root_files = self.sftp.listdir(".")
            logger.info(f"Found {len(root_files)} items in root directory")
            
            # Look for the server ID pattern (IP_serverID or host_serverID)
            for item in root_files:
                if (f"{self.host.split(':')[0]}_{self.server_id}" in item) or (f"host_{self.server_id}" in item):
                    # Found matching directory
                    target_directory = os.path.join(".", item)
                    logger.info(f"Found server directory: {target_directory}")
                    break
            
            if not target_directory:
                logger.error(f"Could not find server directory for server ID: {self.server_id}")
                self.last_error = f"Server directory for ID {self.server_id} not found"
                return []
            
            # Using the specified path structure: host_serverid/actual1/deathlogs
            # We'll first check for the 'actual1' directory, then 'deathlogs'
            logger.info(f"Looking for CSV files in the deathlogs directory structure")
            
            try:
                # Get server directory contents
                server_items = self.sftp.listdir(target_directory)
                logger.info(f"Server directory contains: {', '.join(server_items)}")
                
                # Look for 'actual1' directory
                actual_dir = None
                for item in server_items:
                    if item.lower() == "actual1":
                        actual_dir = os.path.join(target_directory, item)
                        logger.info(f"Found 'actual1' directory: {actual_dir}")
                        break
                
                if not actual_dir:
                    logger.warning("Could not find 'actual1' directory, will search all subdirectories")
                    # Fall back to general search
                    discovered_csv_paths = await self._find_csv_files_recursive(target_directory)
                else:
                    # Found actual1 directory, now look for 'deathlogs'
                    actual_items = self.sftp.listdir(actual_dir)
                    logger.info(f"'actual1' directory contains: {', '.join(actual_items)}")
                    
                    deathlogs_dir = None
                    for item in actual_items:
                        if item.lower() == "deathlogs":
                            deathlogs_dir = os.path.join(actual_dir, item)
                            logger.info(f"Found 'deathlogs' directory: {deathlogs_dir}")
                            break
                    
                    if not deathlogs_dir:
                        logger.warning("Could not find 'deathlogs' directory, will search in 'actual1' and its subdirectories")
                        discovered_csv_paths = await self._find_csv_files_recursive(actual_dir)
                    else:
                        # Found deathlogs directory, look for CSV files here and in subdirectories
                        logger.info(f"Searching for CSV files in deathlogs directory: {deathlogs_dir}")
                        
                        # CRITICAL FIX: Check ALL subdirectories in deathlogs for CSV files, regardless of name
                        logger.info(f"Checking ALL subdirectories in deathlogs directory: {deathlogs_dir}")
                        
                        try:
                            # Get a list of ALL items in deathlogs, don't assume special names
                            deathlogs_items = self.sftp.listdir(deathlogs_dir)
                            logger.info(f"Deathlogs directory contains {len(deathlogs_items)} items: {deathlogs_items}")
                            
                            # First check if there are CSV files directly in deathlogs
                            csv_in_deathlogs = [f for f in deathlogs_items if f.lower().endswith('.csv')]
                            if csv_in_deathlogs:
                                logger.info(f"Found {len(csv_in_deathlogs)} CSV files directly in deathlogs: {csv_in_deathlogs}")
                            
                            # Now check what subdirectories exist
                            subdirs = []
                            for item in deathlogs_items:
                                item_path = os.path.join(deathlogs_dir, item)
                                try:
                                    # Check if it's a directory
                                    if self._is_dir(item_path):
                                        subdirs.append(item_path)
                                        logger.info(f"Found subdirectory in deathlogs: {item_path}")
                                except Exception as e:
                                    logger.warning(f"Error checking if {item_path} is a directory: {e}")
                            
                            # CRITICAL FIX: Use proper list for CSV paths
                            all_csv_paths = []
                            
                            if subdirs:
                                logger.info(f"Checking {len(subdirs)} subdirectories for CSV files")
                                for subdir in subdirs:
                                    try:
                                        # List files in subdirectory
                                        subdir_items = self.sftp.listdir(subdir)
                                        logger.info(f"Subdirectory {subdir} contains {len(subdir_items)} items: {subdir_items}")
                                        
                                        # Check for CSV files
                                        csv_files = [f for f in subdir_items if f.lower().endswith('.csv')]
                                        if csv_files:
                                            logger.info(f"Found {len(csv_files)} CSV files in {subdir}: {csv_files}")
                                            for csv_file in csv_files:
                                                csv_path = os.path.join(subdir, csv_file)
                                                # CRITICAL FIX: DIRECTLY process timestamp for each file here
                                                try:
                                                    # Extract the filename from the path
                                                    logger.info(f"Processing CSV file: {csv_file} at {csv_path}")
                                                    
                                                    # Try to parse timestamp from filename
                                                    timestamp_str = csv_file.split(".csv")[0]
                                                    timestamp = None
                                                    
                                                    # Try various timestamp formats
                                                    formats_to_try = [
                                                        "%Y.%m.%d-%H.%M.%S",
                                                        "%Y-%m-%d_%H-%M-%S",
                                                        "%Y%m%d_%H%M%S",
                                                        "%Y%m%d%H%M%S",
                                                        "%Y-%m-%d"
                                                    ]
                                                    
                                                    for fmt in formats_to_try:
                                                        try:
                                                            timestamp = datetime.datetime.strptime(timestamp_str, fmt)
                                                            logger.info(f"Parsed timestamp using format: {fmt}")
                                                            break
                                                        except ValueError:
                                                            continue
                                                    
                                                    # If we couldn't parse a timestamp, use current time
                                                    if not timestamp:
                                                        logger.warning(f"Could not parse timestamp from {csv_file}, using current time")
                                                        timestamp = datetime.datetime.now()
                                                    
                                                    # CRITICAL FIX: Simply add the path to the list - no tuples
                                                    all_csv_paths.append(csv_path)
                                                    logger.info(f"Added CSV file to list: {csv_path}")
                                                except Exception as ve:
                                                    logger.warning(f"Error processing CSV file {csv_path}: {ve}")
                                                    # Still add to the list even if we have an error
                                                    all_csv_paths.append(csv_path)
                                                    logger.info(f"Added CSV file to list despite error: {csv_path}")
                                    except Exception as se:
                                        logger.warning(f"Error checking subdirectory {subdir}: {se}")
                            
                            # Add any CSV files found directly in deathlogs
                            for csv_file in csv_in_deathlogs:
                                csv_path = os.path.join(deathlogs_dir, csv_file)
                                all_csv_paths.append(csv_path)
                                logger.info(f"Added direct CSV file: {csv_path}")
                                
                            # Use the files we found directly
                            if all_csv_paths:
                                logger.info(f"Successfully found {len(all_csv_paths)} CSV files through direct checking: {all_csv_paths}")
                                discovered_csv_paths = all_csv_paths
                            else:
                                # Fall back to recursive search if we still didn't find any files
                                logger.info(f"No CSV files found via direct check, using recursive search with higher depth")
                                discovered_csv_paths = await self._find_csv_files_recursive(deathlogs_dir, max_depth=8)
                                
                        except Exception as e:
                            logger.error(f"Error directly checking deathlogs subdirectories: {e}")
                            # Fall back to recursive search
                            logger.info(f"Falling back to recursive search with higher depth")
                            discovered_csv_paths = await self._find_csv_files_recursive(deathlogs_dir, max_depth=8)
                    
            except Exception as e:
                logger.error(f"Error exploring directory structure: {e}")
                # Fall back to searching the entire server directory
                logger.info("Falling back to general search in server directory")
                discovered_csv_paths = await self._find_csv_files_recursive(target_directory, max_depth=6)
            
            # Process all discovered CSV files
            logger.info(f"Found a total of {len(discovered_csv_paths)} CSV paths for processing")
            if discovered_csv_paths:
                logger.info(f"CSV paths found in directories: {set([os.path.dirname(p) for p in discovered_csv_paths])}")
            
            for file_path in discovered_csv_paths:
                try:
                    # Extract the filename from the path
                    filename = os.path.basename(file_path)
                    logger.info(f"Processing discovered CSV file: {filename} at {file_path}")
                    
                    # Try to parse timestamp from filename
                    timestamp_str = filename.split(".csv")[0]
                    timestamp = None
                    
                    # Try various timestamp formats
                    formats_to_try = [
                        "%Y.%m.%d-%H.%M.%S",
                        "%Y-%m-%d_%H-%M-%S",
                        "%Y%m%d_%H%M%S",
                        "%Y%m%d%H%M%S",
                        "%Y-%m-%d"
                    ]
                    
                    for fmt in formats_to_try:
                        try:
                            timestamp = datetime.datetime.strptime(timestamp_str, fmt)
                            logger.info(f"Parsed timestamp using format: {fmt}")
                            break
                        except ValueError:
                            continue
                    
                    # If we couldn't parse a timestamp, use current time
                    if not timestamp:
                        logger.warning(f"Could not parse timestamp from {filename}, using current time")
                        timestamp = datetime.datetime.now()
                    
                    csv_files.append((file_path, timestamp, filename))
                except Exception as ve:
                    logger.warning(f"Error processing CSV file {file_path}: {ve}")
                    # Still add with current timestamp
                    timestamp = datetime.datetime.now()
                    filename = os.path.basename(file_path)
                    csv_files.append((file_path, timestamp, filename))
            
            # Sort by timestamp (oldest first) - CRITICAL FIX: Ensure all timestamps are datetime objects
            try:
                # Make sure all timestamps are actually datetime objects
                sanitized_files = []
                for file_tuple in csv_files:
                    file_path, timestamp, filename = file_tuple
                    # If timestamp isn't a datetime, convert it
                    if not isinstance(timestamp, datetime.datetime):
                        logger.warning(f"Converting non-datetime timestamp for {filename}: {timestamp}")
                        timestamp = datetime.datetime.now()
                    sanitized_files.append((file_path, timestamp, filename))
                
                # Now sort with sanitized timestamps
                sanitized_files.sort(key=lambda x: x[1])
                csv_files = sanitized_files
            except Exception as sort_error:
                logger.error(f"Error sorting CSV files: {sort_error}")
                # Continue without sorting if there's an error
            
            if not csv_files:
                logger.warning(f"No CSV files found in any directory or subdirectory")
                self.last_error = "No CSV files found"
            else:
                logger.info(f"Found {len(csv_files)} CSV files across all directories")
                for file_path, timestamp, filename in csv_files:
                    logger.info(f"CSV file: {file_path} (timestamp: {timestamp})")
            
            # Return just the file paths, which is what's expected
            return [file_path for file_path, _, _ in csv_files]
                
        except Exception as e:
            logger.error(f"Error getting all CSV files: {e}", exc_info=True)
            self.last_error = f"Error searching for CSV files: {str(e)}"
            return []
    
    async def _find_csv_files_recursive(self, directory, max_depth=6, current_depth=0, start_time=None, starting_dir=None):
        """Recursively search for CSV files in all subdirectories with timeout protection
        
        Args:
            directory: The directory to search
            max_depth: Maximum recursion depth 
            current_depth: Current recursion depth
            start_time: Start time of the search to prevent timeouts
            starting_dir: IGNORED - We need to search everywhere
        """
        # Set start time on first call to track total execution time
        if start_time is None:
            start_time = asyncio.get_event_loop().time()
            
        # CRITICAL FIX: Explicitly log every check with detailed depth info
        logger.info(f"CSV SEARCH: Checking directory: {directory} (depth {current_depth}/{max_depth})")
        
        # Timeout safety - prevent Discord from disconnecting due to missed heartbeats
        current_time = asyncio.get_event_loop().time()
        if current_time - start_time > 10:  # Increased from 5s to 10s for more thorough searching
            logger.warning(f"CSV SEARCH: Search taking too long, stopping at {directory} to prevent heartbeat timeout")
            return []
            
        # Limit recursion depth to prevent deep searches
        if current_depth > max_depth:
            logger.info(f"CSV SEARCH: Max depth {max_depth} reached at {directory}")
            return []
            
        csv_files = []
        
        try:
            # Get directory contents (with timeout protection)
            try:
                # CRITICAL FIX: Use longer timeout for listing directories
                items = await asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(None, lambda: self.sftp.listdir(directory)),
                    timeout=3.0  # Increased from 2s to 3s for better reliability
                )
                logger.info(f"CSV SEARCH: Directory {directory} contains {len(items)} items")
                # CRITICAL FIX: Always log ALL items in the directory to help with debugging
                logger.info(f"CSV SEARCH: All items in {directory}: {items}")
            except asyncio.TimeoutError:
                logger.warning(f"CSV SEARCH: Timeout while listing directory {directory}, skipping")
                return []
            except Exception as list_e:
                logger.error(f"CSV SEARCH: Error listing directory {directory}: {list_e}")
                return []
            
            # CRITICAL FIX: Process csv files first and be aggressive about matching ANY csv file
            for item in items:
                # Check if it's a CSV file with much more permissive matching
                if item.lower().endswith('.csv'):
                    item_path = os.path.join(directory, item)
                    logger.info(f"CSV SEARCH: Found CSV file: {item} in directory: {directory}")
                    csv_files.append(item_path)
            
            if csv_files:
                logger.info(f"CSV SEARCH: Found {len(csv_files)} CSV files in directory {directory}: {csv_files}")
            
            # CRITICAL FIX: Always process ALL subdirectories with no restrictions
            for item in items:
                # Skip timeout check - we want to explore EVERYTHING
                item_path = os.path.join(directory, item)
                
                try:
                    # Skip if it's a CSV file (already processed)
                    if item.lower().endswith('.csv'):
                        continue
                    
                    # Use simple check if it's a directory
                    is_dir = False
                    try:
                        # CRITICAL FIX: Use direct stat call with increased timeout
                        is_dir = await asyncio.wait_for(
                            asyncio.get_event_loop().run_in_executor(None, lambda: self._is_dir(item_path)),
                            timeout=2.0  # Increased timeout
                        )
                    except Exception:
                        continue
                    
                    if is_dir:
                        # CRITICAL FIX: ALWAYS explore every directory regardless of path
                        logger.info(f"CSV SEARCH: Will explore subdirectory: {item_path} (depth {current_depth})")
                        
                        # CRITICAL FIX: Immediately search the subdirectory
                        subdirectory_files = await self._find_csv_files_recursive(
                            item_path, max_depth, current_depth + 1, start_time
                        )
                        
                        if subdirectory_files:
                            logger.info(f"CSV SEARCH: Found {len(subdirectory_files)} CSV files in {item_path}")
                            csv_files.extend(subdirectory_files)
                            
                except Exception as item_e:
                    logger.warning(f"CSV SEARCH: Error processing subdirectory {item_path}: {str(item_e)}")
                    continue
                        
        except Exception as e:
            logger.error(f"CSV SEARCH: Error in directory {directory}: {str(e)}")
            
        # Return ALL CSV files found
        return csv_files
    
    async def get_log_file(self):
        """Get the path to the Deadside.log file"""
        if not self.connected:
            await self.connect()
        if not self.connected:
            return None
        
        try:
            # Record start time to prevent heartbeat blocks
            start_time = asyncio.get_event_loop().time()
            
            # First find the server directory using specific format {Host}_{serverid}/Logs
            target_directory = None
            
            # Direct path to the logs directory based on the provided structure
            server_dir_pattern = f"{self.host.split(':')[0]}_{self.server_id}"
            logs_dir = os.path.join(".", server_dir_pattern, "Logs")
            
            logger.info(f"Using direct log path: {logs_dir}")
            
            # Check if the logs directory exists with timeout protection
            try:
                async def dir_exists_with_timeout():
                    try:
                        # Try to list the directory to see if it exists
                        items = await asyncio.to_thread(lambda: self.sftp.listdir(logs_dir))
                        return True, items
                    except:
                        return False, None
                
                exists, logs_items = await asyncio.wait_for(dir_exists_with_timeout(), timeout=3.0)
                
                if exists and logs_items:
                    logger.info(f"'Logs' directory contains: {', '.join(logs_items)}")
                    
                    # Check for Deadside.log in the logs directory
                    if LOG_FILENAME in logs_items:
                        log_path = os.path.join(logs_dir, LOG_FILENAME)
                        logger.info(f"Found log file at: {log_path}")
                        return log_path
                    else:
                        logger.warning(f"'{LOG_FILENAME}' not found in direct Logs directory path")
                
            except asyncio.TimeoutError:
                logger.warning(f"Timeout checking direct logs directory: {logs_dir}")
            except Exception as e:
                logger.warning(f"Error with direct logs path: {e}")
            
            # Fallback approach if direct path fails
            if asyncio.get_event_loop().time() - start_time > 15:
                logger.warning("Log file search is taking too long, stopping to prevent heartbeat timeout")
                return None
                
            # List root directory with timeout protection
            logger.info("Fallback: Searching for server directory in root...")
            try:
                async def list_root_with_timeout():
                    return await asyncio.to_thread(lambda: self.sftp.listdir("."))
                
                root_files = await asyncio.wait_for(list_root_with_timeout(), timeout=3.0)
                
                # Look for the server ID pattern
                for item in root_files:
                    if server_dir_pattern in item:
                        # Found matching directory
                        target_directory = os.path.join(".", item)
                        logger.info(f"Found server directory for logs: {target_directory}")
                        
                        # Try to directly access the logs directory now
                        logs_dir = os.path.join(target_directory, "Logs")
                        
                        try:
                            async def check_logs_dir():
                                return await asyncio.to_thread(lambda: self.sftp.listdir(logs_dir))
                            
                            logs_items = await asyncio.wait_for(check_logs_dir(), timeout=3.0)
                            
                            if LOG_FILENAME in logs_items:
                                log_path = os.path.join(logs_dir, LOG_FILENAME)
                                logger.info(f"Found log file at: {log_path}")
                                return log_path
                        except Exception:
                            logger.warning(f"Could not find {LOG_FILENAME} in {logs_dir}")
                        
                        break
                
            except (asyncio.TimeoutError, Exception) as e:
                logger.error(f"Error in fallback log file search: {e}")
            
            # If we've tried everything and still don't have the file
            logger.error("Could not find the log file using any method")
            return None
                
        except Exception as e:
            logger.error(f"Error getting log file: {e}", exc_info=True)
            return None
    
    async def _find_specific_log_file(self, directory, max_depth=2, current_depth=0):
        """Search specifically for Deadside.log in the directory structure"""
        if current_depth > max_depth:
            return None
        
        try:
            # First check if Deadside.log exists in this directory
            try:
                items = self.sftp.listdir(directory)
                
                if LOG_FILENAME in items:
                    log_path = os.path.join(directory, LOG_FILENAME)
                    logger.info(f"Found {LOG_FILENAME} in directory: {directory}")
                    return log_path
            except Exception as list_e:
                logger.warning(f"Error listing directory {directory}: {list_e}")
                return None
            
            # If not found, check subdirectories
            for item in items:
                # Only look for directories named 'Logs' or any directory if we're at depth 0
                if item.lower() == "logs" or current_depth == 0:
                    item_path = os.path.join(directory, item)
                    
                    try:
                        if self._is_dir(item_path):
                            # First check this directory for the log file
                            subdir_items = self.sftp.listdir(item_path)
                            
                            if LOG_FILENAME in subdir_items:
                                log_path = os.path.join(item_path, LOG_FILENAME)
                                logger.info(f"Found {LOG_FILENAME} in subdirectory: {item_path}")
                                return log_path
                            
                            # If not found, recurse deeper
                            log_path = await self._find_specific_log_file(
                                item_path, max_depth, current_depth + 1
                            )
                            if log_path:
                                return log_path
                    except Exception as e:
                        logger.warning(f"Error processing subdirectory {item_path}: {e}")
                        continue
            
            # Not found in this directory or its subdirectories
            return None
                
        except Exception as e:
            logger.error(f"Error searching for {LOG_FILENAME}: {e}")
            return None
    
    async def read_file(self, file_path, start_line=0, max_lines=None, chunk_size=1000):
        """Read a file from the SFTP server with timeout protection
        
        Args:
            file_path: Path to the file to read
            start_line: First line to read (0-indexed)
            max_lines: Maximum number of lines to read
            chunk_size: Number of lines to read in each chunk (to prevent timeout)
            
        Returns:
            List of lines read from the file
        """
        if not self.connected:
            await self.connect()
        if not self.connected:
            return []
        
        try:
            # Create a new task with timeout to prevent event loop blocking
            async def read_chunk(start, max_count):
                try:
                    # We'll create a new file handle for each chunk to avoid timeout issues
                    with self.sftp.file(file_path, 'r') as chunk_file:
                        # Skip to the starting position
                        for _ in range(start):
                            next(chunk_file, None)
                            
                        # Read the requested chunk
                        chunk_lines = []
                        count = 0
                        
                        # Use a separate thread for potentially blocking IO
                        # and add a timeout to prevent blocking the event loop
                        return await asyncio.wait_for(
                            asyncio.to_thread(self._read_chunk, chunk_file, max_count),
                            timeout=5.0  # 5 second timeout
                        )
                        
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout reading file {file_path} at position {start}")
                    # Try to reconnect on timeout
                    await self.disconnect()
                    await asyncio.sleep(1)
                    await self.connect()
                    return []
                except Exception as e:
                    logger.error(f"Error reading chunk at position {start}: {e}")
                    return []
            
            all_lines = []
            current_position = start_line
            remaining_lines = max_lines
            
            while True:
                # Determine how many lines to read in this chunk
                current_chunk_size = chunk_size
                if remaining_lines is not None:
                    if remaining_lines <= 0:
                        break
                    current_chunk_size = min(chunk_size, remaining_lines)
                
                # Read the next chunk with timeout protection
                chunk_data = await read_chunk(current_position, current_chunk_size)
                
                # Break if we got no data (end of file or error)
                if not chunk_data:
                    break
                    
                # Update our tracking variables
                all_lines.extend([line.strip() for line in chunk_data])
                chunk_line_count = len(chunk_data)
                current_position += chunk_line_count
                
                if remaining_lines is not None:
                    remaining_lines -= chunk_line_count
                    
                # If we got fewer lines than requested, we've reached the end of the file
                if chunk_line_count < current_chunk_size:
                    break
                    
                # Add a small delay to prevent overloading the event loop
                await asyncio.sleep(0.01)
            
            return all_lines
                
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}", exc_info=True)
            # Try to reconnect after an error
            await self.disconnect()
            await asyncio.sleep(1)
            await self.connect()
            return []
            
    def _read_chunk(self, file_obj, max_lines):
        """Read a chunk of lines from a file (runs in a separate thread)
        
        Args:
            file_obj: Open file object
            max_lines: Maximum number of lines to read
            
        Returns:
            List of lines read
        """
        try:
            lines = []
            for _ in range(max_lines):
                try:
                    line = next(file_obj)
                    lines.append(line)
                except StopIteration:
                    break
            return lines
        except Exception as e:
            logger.error(f"Error in _read_chunk: {e}")
            return []
    
    async def get_file_size(self, file_path, chunk_size=5000):
        """Get the size of a file in lines with timeout protection
        
        Args:
            file_path: Path to the file
            chunk_size: Number of lines to count in each chunk (to prevent timeout)
            
        Returns:
            Number of lines in the file
        """
        # Record start time to prevent heartbeat blocks
        start_time = asyncio.get_event_loop().time()
        
        if not self.connected:
            await self.connect()
        if not self.connected:
            return 0
        
        try:
            # Add global timeout protection for the entire method
            # Return an estimate if taking too long
            if asyncio.get_event_loop().time() - start_time > 20:
                logger.warning(f"get_file_size already taking too long, using default estimation")
                return 5000  # Default estimate to prevent heartbeat blocks
            
            # OPTIMIZATION: Use a more efficient method to count lines
            # This runs in a background thread to avoid blocking the event loop
            try:
                # We'll use Python's file iterator which is much faster for counting lines
                # This will be executed in a separate thread to prevent blocking
                def count_lines():
                    try:
                        with self.sftp.file(file_path, 'r') as f:
                            # Count lines in batches for very large files
                            count = 0
                            # Set a maximum number of lines to count to prevent extreme timeouts
                            # For very large files, we'll fall back to estimation anyway
                            max_count = 50000
                            
                            for i, _ in enumerate(f):
                                count = i + 1
                                # Every chunk_size lines, yield to allow checking for timeout
                                if count % chunk_size == 0:
                                    # Just continue the loop
                                    pass
                                    
                                # Break early for extremely large files
                                if count >= max_count:
                                    # Add 10% to indicate it's larger
                                    logger.warning(f"File {file_path} exceeds {max_count} lines, using estimate")
                                    return int(count * 1.1)
                                    
                            return count
                    except Exception as e:
                        logger.error(f"Error counting lines in thread: {e}")
                        return 0
                        
                # Run the counting function in a thread with a timeout
                logger.debug(f"Counting lines in {file_path} with timeout protection")
                return await asyncio.wait_for(
                    asyncio.to_thread(count_lines),
                    timeout=8.0  # Shorter timeout to prevent heartbeat blocks
                )
                
            except asyncio.TimeoutError:
                logger.warning(f"Timeout while counting lines in {file_path}, falling back to estimation")
                # Fall back to an estimation method on timeout
                
                # Add another heartbeat protection check
                if asyncio.get_event_loop().time() - start_time > 20:
                    logger.warning(f"get_file_size taking too long during estimation, using default value")
                    return 5000  # Default estimate to prevent heartbeat blocks
                
                try:
                    # Get file attributes with timeout protection
                    # Use run_in_executor to make sure SFTP stat operation doesn't block
                    async def get_stat_with_timeout():
                        return await asyncio.to_thread(lambda: self.sftp.stat(file_path))
                    
                    # Add a timeout for the stat operation itself
                    try:
                        attrs = await asyncio.wait_for(get_stat_with_timeout(), timeout=3.0)
                        # Estimate based on file size - assume 100 bytes per line as a rough guess
                        # This is better than nothing if the full count times out
                        estimated_lines = attrs.st_size // 100
                        logger.info(f"File {file_path} has estimated {estimated_lines} lines based on size {attrs.st_size} bytes")
                        return estimated_lines
                    except asyncio.TimeoutError:
                        logger.warning(f"Timeout getting file stats for {file_path}, using default estimation")
                        return 5000  # Default if stat times out
                        
                except Exception as est_error:
                    logger.error(f"Error estimating file size: {est_error}")
                    return 5000  # Arbitrary fallback if all else fails
                    
        except Exception as e:
            logger.error(f"Error in get_file_size: {e}", exc_info=True)
            return 0
    
    @staticmethod
    def run_in_executor(func, *args, **kwargs):
        """Run a blocking function in an executor"""
        loop = asyncio.get_event_loop()
        return loop.run_in_executor(None, lambda: func(*args, **kwargs))
